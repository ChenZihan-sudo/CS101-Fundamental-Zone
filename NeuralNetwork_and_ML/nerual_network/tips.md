# Nerual Network

## 全连接层

https://zhuanlan.zhihu.com/p/552186222

https://blog.csdn.net/weixin_42782833/article/details/118255739

## BN层

https://blog.csdn.net/weixin_42782833/article/details/118255739

## DenseBlock

https://www.python100.com/html/108001.html

https://www.sohu.com/a/430820458_455817

## Backbone

backbone这个单词原意指的是人的脊梁骨，后来引申为支柱，核心的意思，在神经网络中，指的是网络的主干网络。

## 激活函数

### Sigmoid

https://cloud.tencent.com/developer/article/1092667

https://zhuanlan.zhihu.com/p/424858561

Sigmoid函数是 Logistic函数的一种特殊形式，通常用 σ (x)或 sig (x)来表示。如下所示：

o(x)=1/(1+exp⁡(−x))

Sigmoid 函数是一条 s 形曲线，如下图中的绿线所示。该图还显示了粉红色的导数图形：

![img](.\pics\v2-f85d8dcaa50d39f38f7b76a85ce0da4b_1440w.webp)

#### **压缩函数**

Sigmoid函数也可以作为压缩函数，因为它的域是所有实数的集合，它的范围是(0,1)。因此，如果函数的输入是一个非常大的负数或非常大的正数，则输出总是介于0和1之间。在-∞和 + ∞之间的任何数字也是如此。

#### **激活函数**

Sigmoid函数被用作神经网络中的激活函数。为了回顾什么是激活函数神经元，下面的图显示了激活函数神经元在神经网络的一个层中所起的作用。输入的加权和通过一个激活函数，这个输出作为下一层的输入。

![img](.\pics\v2-2e5fc8e88e1989749fc966a825cd5664_1440w.webp)

#### 为什么 Sigmoid函数在神经网络中很重要?

如果我们在神经网络中使用线性激活函数，那么这个模型只能学习线性可分问题。然而，**只要在隐藏层中增加一个隐藏层和一个 Sigmoid激活函数，神经网络就可以很容易地学习一个非线性可分问题** 。使用非线性函数产生非线性边界，因此，Sigmoid函数可以用于神经网络学习复杂的决策函数。

在神经网络中，可以用作激活函数的非线性函数必须是一个**单调递增** 的函数。例如，sin (x)或 cos (x)不能用作激活函数。另外，激活函数应该**定义为任意处** ，并且**在实数空间中任意处都是连续的** 。这个函数还要求在整个实数空间上是**可微** 的。

通常，反向传播算法使用梯度下降法学习神经网络的权重。为了得到这个算法，需要对激活函数进行求导数。而由于Sigmoid函数是**单调的，连续的，到处可微的** ，再加上**它的导数可以用它自己来表示** 的特性，使得当使用反向传播算法时，很容易推导出学习神经网络中的权重的更新方程。

### ReLU

第一，采用sigmoid等函数，算激活函数时（指数运算），**计算量大**，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。

第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现 **梯度消失** 的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练。

第三，ReLu会使一部分[神经元](https://link.zhihu.com/?target=https%3A//www.baidu.com/s%3Fwd%3D%E7%A5%9E%E7%BB%8F%E5%85%83%26tn%3D24004469_oem_dg%26rsv_dl%3Dgh_pl_sl_csd)的输出为0，这样就造成了 **网络的稀疏性**，并且减少了参数的相互依存关系，**缓解了过拟合**问题的发生。

https://zhuanlan.zhihu.com/p/428448728

### Softmax

https://cloud.tencent.com/developer/article/1092667

https://zhuanlan.zhihu.com/p/628492966



## 池化层

https://zhuanlan.zhihu.com/p/545293528

https://zhuanlan.zhihu.com/p/393761765

**池化**一般接在卷积过程后。

池化，也叫Pooling，其本质其实就是采样，池化对于输入的图片，选择某种方式对其进行压缩，以加快神经网络的运算速度。这里说的某种方式，其实就是池化的算法，比如最大池化或平均池化。

### **特征不变性**

通过池化操作，图片中的特征在输出图片中，仍然被保留了下来，虽然有些许的误差。

### **降维**

如上的例子，图片经过池化的操作，可以减小图片的尺寸，同时又可以保留相应特征，所以主要用来降维。

### **防止过拟合**

由于池化层没有需要学习的参数，因此，在训练的过程中，可以在一定程度上防止过拟合的发生。

### **降低模型计算量**

池化的操作，会在保留原始图片特征不变的情况下，将图片尺寸缩小，从而减少整个模型的计算量。

### SPP(Spatial Pyramid Pooling) 空间金字塔池化层

https://blog.csdn.net/qq_36926037/article/details/105310907

**损失函数**用来评价模型的**预测值**和**真实值**不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

**损失函数**分为**经验风险损失函数**和**结构风险损失函数**。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。
避免图像裁剪

**SPP的出现：使得CNN网络不再需要固定尺寸图像的输入，这是一项重大突破。**

SSP层的作用是使CNN能够输入任意大小的图片，在CNN的最后一层卷积层后面加入一层SSP层，他能使不同任意尺寸的特征图通过SSP层之后都能输出一个固定长度的向量。然后将这个固定长度的向量输入到全连接层，进行后续的分类检测任务。

![在这里插入图片描述](.\pics\watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2OTI2MDM3,size_16,color_FFFFFF,t_70)

## 空洞卷积

![image-20231010200915512](.\pics\image-20231010200915512.png)
